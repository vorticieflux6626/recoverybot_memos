# vLLM High-Throughput Optimization Theory Report

**Date**: 2026-01-12
**Based on**: FANUC SRVO-062 query benchmark (195s, 73% confidence)
**Scope**: Agentic pipeline + VL scraping LLM call analysis

---

## Executive Summary

Analysis of the current Ollama-based pipeline reveals **10 LLM calls per search query** using **4 distinct models**. The sequential nature of Ollama requests creates a bottleneck where:

- **gemma3:4b** handles 60% of calls (6/10)
- **qwen3:8b** consumes 45% of LLM time (35.9s of 79.2s)
- Model loading/unloading overhead adds ~2-5s per model switch

**Theoretical speedup with vLLM**: **2.8x-4.2x** (79s â†’ 19-28s for LLM operations)

---

## Current Pipeline LLM Call Analysis

### Per-Query Model Usage

| Model | Calls | Time (ms) | % Time | Phases |
|-------|-------|-----------|--------|--------|
| **gemma3:4b** | 6 | 33,734 | 42.6% | Analysis, Planning, URL Filter, Verify (Ã—3) |
| **qwen3:8b** | 2 | 35,913 | 45.4% | HyDE, Synthesis |
| **qwen3:4b-instruct-q8_0** | 1 | 4,513 | 5.7% | CRAG Evaluation |
| **cogito:8b** | 1 | 5,018 | 6.3% | Self-Reflection |
| **TOTAL** | **10** | **79,178** | 100% | |

### VL Scraping (Additional)

| Model | Calls/Query | Time/Call | Usage Rate |
|-------|-------------|-----------|------------|
| qwen2.5-vl:7b | ~6 (of 20 URLs) | 8,000ms | ~30% of URLs |

With VL scraping: **+48,000ms** potential (6 Ã— 8s)

---

## Ollama vs vLLM Architecture Comparison

### Current: Ollama (Sequential)

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                        OLLAMA SERVER                            â”‚
â”‚                                                                 â”‚
â”‚  Request 1 â”€â”€â”€â”€â”€â–º [Load gemma3:4b] â”€â”€â”€â”€â”€â–º Generate â”€â”€â”€â”€â”€â–º      â”‚
â”‚                          â†“                                      â”‚
â”‚  Request 2 â”€â”€â”€â”€â”€â–º [Unload/Load qwen3:8b] â”€â”€â”€â”€â”€â–º Generate â”€â”€â”€â”€â”€â–ºâ”‚
â”‚                          â†“                                      â”‚
â”‚  Request 3 â”€â”€â”€â”€â”€â–º [Keep qwen3:8b] â”€â”€â”€â”€â”€â–º Generate â”€â”€â”€â”€â”€â–º       â”‚
â”‚                          â†“                                      â”‚
â”‚  Request 4 â”€â”€â”€â”€â”€â–º [Unload/Load cogito:8b] â”€â”€â”€â”€â”€â–º Generate â”€â”€â”€â”€â”€â–ºâ”‚
â”‚                                                                 â”‚
â”‚  âš ï¸ Model switching overhead: 2-5s per switch                   â”‚
â”‚  âš ï¸ Sequential processing: No parallelism                       â”‚
â”‚  âš ï¸ Single GPU utilization: ~60-70%                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Timeline (79s LLM time):
[====gemma3:4b====][==qwen3:8b==][==qwen3:8b==][cogito][q8_0]
     33.7s              5s          30.9s       5s     4.5s
```

### Proposed: vLLM (Continuous Batching)

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                     vLLM INFERENCE SERVER                       â”‚
â”‚                                                                 â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚              ALWAYS-LOADED MODEL POOL                     â”‚  â”‚
â”‚  â”‚                                                           â”‚  â”‚
â”‚  â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”‚  â”‚
â”‚  â”‚   â”‚  gemma3:4b  â”‚  â”‚  qwen3:8b   â”‚  â”‚  cogito:8b  â”‚     â”‚  â”‚
â”‚  â”‚   â”‚   (3.2GB)   â”‚  â”‚   (5.6GB)   â”‚  â”‚   (4.5GB)   â”‚     â”‚  â”‚
â”‚  â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â”‚  â”‚
â”‚  â”‚                                                           â”‚  â”‚
â”‚  â”‚   Total VRAM: ~13.3GB (fits in 24GB GPU)                 â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚                                                                 â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚              CONTINUOUS BATCHING ENGINE                   â”‚  â”‚
â”‚  â”‚                                                           â”‚  â”‚
â”‚  â”‚  Incoming requests â†’ Dynamic batching â†’ Parallel decode  â”‚  â”‚
â”‚  â”‚                                                           â”‚  â”‚
â”‚  â”‚  â€¢ Batch size: 8-32 requests                             â”‚  â”‚
â”‚  â”‚  â€¢ PagedAttention: Efficient KV cache                    â”‚  â”‚
â”‚  â”‚  â€¢ Speculative decoding: 2x token throughput             â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚                                                                 â”‚
â”‚  âœ… No model loading overhead                                   â”‚
â”‚  âœ… Parallel inference across models                            â”‚
â”‚  âœ… GPU utilization: 90-95%                                     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Timeline (theoretical 19-28s):
[gemma3:4bâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€]  (parallel phases)
[qwen3:8bâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€]  (batched HyDE+Synth)
[cogito:8bâ”€â”€]                   (no load overhead)
[q8_0â”€]
```

---

## Optimization Strategies

### Strategy 1: Model Consolidation (Reduce Distinct Models)

**Current**: 4 distinct models
**Proposed**: 2-3 models

| Current Model | Proposed Replacement | Rationale |
|---------------|---------------------|-----------|
| gemma3:4b | qwen3:8b | Consolidate to single high-quality model |
| qwen3:4b-instruct-q8_0 | qwen3:8b | Same family, better quality |
| cogito:8b | qwen3:8b OR keep | cogito:8b has unique thinking capability |

**Benefit**: Fewer model switches, better batching opportunities

**Trade-off**: Slightly slower per-call for simple tasks (8B vs 4B)

### Strategy 2: Parallel Phase Execution

Currently sequential phases that could run in parallel:

```
CURRENT (Sequential):
[Analyze] â†’ [Plan] â†’ [Search] â†’ [Scrape] â†’ [Evaluate] â†’ [Verify] â†’ [Synthesize] â†’ [Reflect]
   8.7s      3s        10s       145s        4.5s        16s         31s           5s

OPTIMIZED (Parallel where possible):
[Analyze + Plan]  â†’ [Search] â†’ [Scrape + Evaluate(batch)] â†’ [Verify(batch)] â†’ [Synthesize] â†’ [Reflect]
     5s               10s              145s                     8s              25s            5s
                                       (parallel VL)
```

**Parallelizable Pairs**:
1. Query Analysis + Search Planning (same model, can batch)
2. URL Evaluation + Scraping (evaluation runs while scraping starts)
3. Verification claims (currently 3 calls â†’ 1 batched call)

### Strategy 3: vLLM Continuous Batching

vLLM's key advantage is **continuous batching** where:

1. **No model loading**: Models stay in VRAM permanently
2. **Request batching**: Multiple requests processed together
3. **Speculative decoding**: Draft model generates candidates, main model verifies

**Configuration for our pipeline**:

```python
# vLLM server configuration
vllm_config = {
    "models": [
        {"name": "gemma3:4b", "tensor_parallel": 1, "gpu_memory_utilization": 0.25},
        {"name": "qwen3:8b", "tensor_parallel": 1, "gpu_memory_utilization": 0.35},
        {"name": "cogito:8b", "tensor_parallel": 1, "gpu_memory_utilization": 0.30},
    ],
    "max_batch_size": 16,
    "enable_speculative_decoding": True,
    "speculative_draft_model": "qwen3:4b",  # Fast draft for qwen3:8b
}
```

### Strategy 4: VL Model Batching

Current VL scraping is **per-URL sequential**. With vLLM:

```
CURRENT (20 URLs, 30% need VL = 6 VL calls):
[VL-1] â†’ [VL-2] â†’ [VL-3] â†’ [VL-4] â†’ [VL-5] â†’ [VL-6]
  8s       8s       8s       8s       8s       8s    = 48s total

VLLM BATCHED:
[VL-1, VL-2, VL-3, VL-4, VL-5, VL-6]  (single batch)
              12-15s total              = 3x speedup
```

---

## Theoretical Speedup Calculations

### LLM Operations Only (79s â†’ 19-28s)

| Optimization | Savings | Remaining |
|--------------|---------|-----------|
| **Baseline** | - | 79s |
| Eliminate model loading (4 switches Ã— 3s) | -12s | 67s |
| Parallel analysis+planning | -6s | 61s |
| Batch verification (3â†’1 call) | -11s | 50s |
| Continuous batching efficiency (+40%) | -20s | 30s |
| Speculative decoding synthesis (+30%) | -9s | 21s |

**Theoretical LLM time**: **19-28s** (2.8x-4.2x speedup)

### Full Pipeline (195s â†’ 85-110s)

| Phase | Current | Optimized | Savings |
|-------|---------|-----------|---------|
| LLM Operations | 79s | 21s | 58s |
| Scraping (VL batched) | 145s | 100s | 45s |
| Search/Network | 10s | 10s | 0s |
| Overhead | 5s | 3s | 2s |
| **TOTAL** | **195s** | **~100s** | **~49% faster** |

---

## Implementation Roadmap

### Phase 1: Model Consolidation (Low Risk)
**Effort**: 2-4 hours
**Speedup**: 10-15%

1. Consolidate gemma3:4b and qwen3:4b-instruct-q8_0 â†’ qwen3:8b
2. Keep cogito:8b for self-reflection (unique capability)
3. Result: 2 models instead of 4

### Phase 2: Request Batching (Medium Risk)
**Effort**: 1-2 days
**Speedup**: 20-30%

1. Batch verification claims (3 calls â†’ 1)
2. Parallel analyze + plan
3. Implement request queue with batching

### Phase 3: vLLM Migration (High Effort)
**Effort**: 1-2 weeks
**Speedup**: 40-60%

1. Deploy vLLM server alongside Ollama
2. Create abstraction layer (gateway) for model routing
3. Migrate high-throughput models to vLLM
4. Keep Ollama for development/fallback

### Phase 4: VL Batching (Medium Effort)
**Effort**: 3-5 days
**Speedup**: 15-20% (of scraping time)

1. Collect screenshots in parallel
2. Batch VL inference (requires vLLM with vision support OR separate VL server)
3. Async result collection

---

## VRAM Requirements

### Current (Ollama, Sequential Loading)
- Peak: ~8GB (one model at a time + overhead)
- Average: ~5GB

### Proposed (vLLM, All Models Loaded)
| Model | VRAM |
|-------|------|
| gemma3:4b | 3.2GB |
| qwen3:8b | 5.6GB |
| cogito:8b | 4.5GB |
| qwen2.5-vl:7b (optional) | 5.0GB |
| **Overhead** | 2.0GB |
| **Total** | **15.3-20.3GB** |

**Requirement**: 24GB GPU (RTX 4090, A6000, etc.)

### Multi-GPU Option
With 2Ã— 12GB GPUs:
- GPU 0: gemma3:4b + cogito:8b (7.7GB)
- GPU 1: qwen3:8b + vl:7b (10.6GB)

---

## Benchmark Predictions

### After Full Optimization (vLLM + Batching)

| Metric | Current | Predicted | Change |
|--------|---------|-----------|--------|
| Total Time | 195s | 85-110s | **-44% to -56%** |
| LLM Time | 79s | 19-28s | **-65% to -76%** |
| Throughput | 0.3 queries/min | 0.7 queries/min | **+133%** |
| GPU Utilization | 60-70% | 90-95% | **+35%** |
| VRAM Usage | 5-8GB (peak) | 15-20GB (constant) | +150% |

### Cost-Benefit Analysis

| Optimization | Effort | Speedup | ROI |
|--------------|--------|---------|-----|
| Model Consolidation | 2h | 15% | â­â­â­â­â­ |
| Request Batching | 2d | 25% | â­â­â­â­ |
| vLLM Migration | 2w | 50% | â­â­â­ |
| VL Batching | 5d | 15% | â­â­ |

---

## Conclusion

The current Ollama-based pipeline has significant optimization potential through:

1. **Model consolidation**: Reduce 4 models to 2-3 (-15% time)
2. **Request batching**: Parallel/batched LLM calls (-25% time)
3. **vLLM migration**: Continuous batching, no loading (-50% time)
4. **VL batching**: Parallel screenshot analysis (-15% scraping time)

**Total theoretical improvement**: 44-56% faster pipeline (195s â†’ 85-110s)

The highest ROI optimization is **model consolidation** followed by **request batching**, both achievable without major infrastructure changes. vLLM migration offers the largest speedup but requires dedicated GPU resources and infrastructure investment.

---

## Appendix: Model Call Sequence Diagram

```
Search Query: "FANUC SRVO-062 troubleshooting"
              â”‚
              â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ PHASE 1: QUERY ANALYSIS                                         â”‚
â”‚ Model: gemma3:4b | Call #1 | 8.7s                               â”‚
â”‚ Input: Query text (500 tokens)                                  â”‚
â”‚ Output: Query type, complexity, search strategy (800 tokens)    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
              â”‚
              â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ PHASE 2: SEARCH PLANNING                                        â”‚
â”‚ Model: gemma3:4b | Call #2 | 3.0s                               â”‚
â”‚ Input: Analysis result (800 tokens)                             â”‚
â”‚ Output: Search plan, queries (400 tokens)                       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
              â”‚
              â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ PHASE 3: HYDE EXPANSION                                         â”‚
â”‚ Model: qwen3:8b | Call #3 | 5.0s                                â”‚
â”‚ Input: Query (300 tokens)                                       â”‚
â”‚ Output: Hypothetical document (500 tokens)                      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
              â”‚
              â–¼
        [Web Search - 10s, no LLM]
              â”‚
              â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ PHASE 4: CRAG EVALUATION                                        â”‚
â”‚ Model: qwen3:4b-instruct-q8_0 | Call #4 | 4.5s                  â”‚
â”‚ Input: Search results (3000 tokens)                             â”‚
â”‚ Output: Quality assessment (300 tokens)                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
              â”‚
              â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ PHASE 5: URL RELEVANCE FILTER                                   â”‚
â”‚ Model: gemma3:4b | Call #5 | 5.7s                               â”‚
â”‚ Input: URLs + snippets (2000 tokens)                            â”‚
â”‚ Output: Filtered URLs (200 tokens)                              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
              â”‚
              â–¼
        [Web Scraping - 145s, includes VL calls]
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚ VL Calls (30% of URLs = ~6 calls)       â”‚
        â”‚ Model: qwen2.5-vl:7b | 6 Ã— 8s = 48s    â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
              â”‚
              â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ PHASE 6: CONTENT VERIFICATION                                   â”‚
â”‚ Model: gemma3:4b | Calls #6-8 | 3 Ã— 5.4s = 16.2s               â”‚
â”‚ Input: Claims to verify (1500 tokens each)                      â”‚
â”‚ Output: Verification results (400 tokens each)                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
              â”‚
              â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ PHASE 7: SYNTHESIS                                              â”‚
â”‚ Model: qwen3:8b | Call #9 | 30.9s                               â”‚
â”‚ Input: All context (40000 tokens)                               â”‚
â”‚ Output: Synthesized answer (2000 tokens)                        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
              â”‚
              â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ PHASE 8: SELF-REFLECTION                                        â”‚
â”‚ Model: cogito:8b | Call #10 | 5.0s                              â”‚
â”‚ Input: Answer + sources (3000 tokens)                           â”‚
â”‚ Output: Quality assessment (500 tokens)                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
              â”‚
              â–¼
        [Response - Total: 195s]
```

---

## Implementation Path: LLM Gateway Sub-Project

> **Note**: The optimizations described in this report are planned for implementation via the **LLM Gateway** sub-project, not direct modification of the memOS agentic pipeline.

### Gateway Architecture (Port 8100)

The Gateway sub-project (`/home/sparkone/sdd/Recovery_Bot/gateway/`) provides a unified routing layer that will implement vLLM integration:

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                          LLM GATEWAY SERVICE                                  â”‚
â”‚                             Port 8100                                         â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                               â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚                      REQUEST ROUTER                                      â”‚ â”‚
â”‚  â”‚  â€¢ API format detection (Ollama native vs OpenAI-compatible)            â”‚ â”‚
â”‚  â”‚  â€¢ Source system identification (memOS, Android, PDF Tools)             â”‚ â”‚
â”‚  â”‚  â€¢ Request batching and queueing                                        â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”‚                                    â”‚                                          â”‚
â”‚              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                   â”‚
â”‚              â–¼                                           â–¼                    â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”          â”‚
â”‚  â”‚   OLLAMA BACKEND      â”‚                â”‚    vLLM BACKEND       â”‚          â”‚
â”‚  â”‚   (Port 11434)        â”‚                â”‚    (Port 8000)        â”‚          â”‚
â”‚  â”‚   âœ… Current primary   â”‚                â”‚    ðŸŽ¯ Future target   â”‚          â”‚
â”‚  â”‚   â€¢ Sequential        â”‚                â”‚    â€¢ Continuous batch â”‚          â”‚
â”‚  â”‚   â€¢ Model swapping    â”‚                â”‚    â€¢ PagedAttention   â”‚          â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜          â”‚
â”‚                                                                               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Migration Strategy

When vLLM is deployed via the Gateway, the migration will be **transparent** to the agentic pipeline:

| Phase | Action | Impact on memOS |
|-------|--------|-----------------|
| **Phase 1** | Gateway intercepts all LLM requests | No code changes required |
| **Phase 2** | High-throughput models route to vLLM | Automatic speedup |
| **Phase 3** | Request batching enabled | Further optimization |
| **Phase 4** | Full vLLM migration (Ollama fallback) | Maximum performance |

### Gateway Integration Points

The Gateway will automatically apply the optimizations from this report:

```python
# Gateway routing logic (conceptual)
class ModelRouter:
    def route_request(self, request: GatewayRequest) -> Backend:
        model = request.model

        # High-throughput models â†’ vLLM (when available)
        if model in ["qwen3:8b", "gemma3:4b"] and self.vllm_available:
            return Backend.VLLM

        # Vision models â†’ Ollama (vLLM vision support limited)
        if "vl" in model or "vision" in model:
            return Backend.OLLAMA

        # Thinking models â†’ Ollama (better streaming support)
        if model in ["cogito:8b", "deepseek-r1:8b"]:
            return Backend.OLLAMA

        # Default: Ollama
        return Backend.OLLAMA
```

### Benchmark-Optimized Model Routing Table

Based on the benchmarks from 2026-01-12, the Gateway will route as follows:

| Model | Backend | Rationale |
|-------|---------|-----------|
| `gemma3:4b` | **vLLM** | 6 calls/query, highest frequency |
| `qwen3:8b` | **vLLM** | Synthesis (30s), benefits from batching |
| `qwen3:4b-instruct-2507-q8_0` | **vLLM** | Fast evaluator, good batch candidate |
| `cogito:8b` | Ollama | Thinking model, streaming preferred |
| `qwen2.5-vl:7b` | Ollama | Vision model, vLLM support limited |

### Expected Timeline

| Milestone | Status | ETA |
|-----------|--------|-----|
| Gateway v0.1 (Ollama proxy) | âœ… Complete | - |
| Gateway v0.2 (vLLM backend) | ðŸ”„ Planned | TBD |
| Request batching | ðŸ”„ Planned | TBD |
| Full vLLM migration | ðŸ”„ Planned | TBD |

### No Action Required for memOS

Once the Gateway implements vLLM, the speedups described in this report will be achieved **without any changes to the memOS agentic pipeline**. The pipeline currently calls Ollama directly via `http://localhost:11434`, which can be redirected to the Gateway (`http://localhost:8100`) via:

1. **Environment variable**: `OLLAMA_BASE_URL=http://localhost:8100/ollama`
2. **Config update**: `llm_config.yaml` â†’ `ollama.url: http://localhost:8100/ollama`

The Gateway will handle all routing, batching, and backend selection transparently.

---

*Report generated from benchmark data: 2026-01-12*
*Gateway integration planned via: `/home/sparkone/sdd/Recovery_Bot/gateway/`*
