# =============================================================================
# LLM Model Configuration for Agentic Pipeline
# =============================================================================
# This file centralizes all LLM model assignments for the memOS agentic search
# pipeline. Changes here affect the entire system's model usage.
#
# Location: /home/sparkone/sdd/Recovery_Bot/memOS/server/config/llm_models.yaml
# Dashboard Access: GET/PUT /api/v1/config/llm-models
#
# Model Selection Criteria:
#   - Speed: Lower latency = faster pipeline (important for interactive use)
#   - Quality: Higher accuracy = better results (important for synthesis)
#   - VRAM: Lower usage = can run more concurrent models
#   - Cost: Smaller models = less computational overhead
#
# Recommended Models by Task Type (Benchmark Results 2026-01-12):
#   - Fastest utility: qwen3:4b-instruct-2507-q8_0 (4.2s avg, 0.83 accuracy)
#   - Balanced utility: gemma3:4b (5.1s avg, 0.91 accuracy)
#   - Quality synthesis: qwen3:8b (12.3s avg, 1.0 accuracy)
#   - Complex judgment: qwen3:30b-a3b-instruct-2507-q4_K_M (10.5s avg, 1.0 accuracy)
#   - Self-reflection: cogito:8b (5.0s, 1.0 accuracy)
#   - Complex reasoning: deepseek-r1:14b, phi4-reasoning:14b
#   - Embeddings: qwen3-embedding, nomic-embed-text
# =============================================================================

version: "1.2.0"
last_updated: "2026-01-12"

# Ollama server configuration
ollama:
  url: "http://localhost:11434"
  default_timeout: 120
  keep_alive: "30m"

# =============================================================================
# PIPELINE STAGE MODELS
# These are the primary models used at each stage of the agentic pipeline
# =============================================================================

pipeline:
  # Query Analysis - determines query type, complexity, search strategy
  # Benchmark: gemma3:4b (0.91 acc, 5091ms) vs qwen3:8b (1.0 acc, 12320ms)
  analyzer:
    model: "gemma3:4b"
    context_window: 131072
    temperature: 0.3
    max_tokens: 2048
    description: "Query analysis, classification, search planning"
    notes: "Benchmark winner: 0.91 accuracy at 2.4x speed of qwen3:8b"

  # URL Evaluation - determines which search results to scrape
  # Benchmark: qwen3:30b-a3b (1.0 acc, 10423ms) vs gemma3:4b (0.66 acc, 5712ms)
  url_evaluator:
    model: "gemma3:4b"
    context_window: 131072
    temperature: 0.3
    max_tokens: 2048
    description: "Evaluate search result relevance for scraping"
    notes: "Balanced: gemma3:4b (0.66 acc). Quality: use qwen3:30b-a3b (1.0 acc)"

  # Content Coverage - evaluates if scraped content answers the query
  coverage_evaluator:
    model: "qwen3:8b"
    context_window: 40960
    temperature: 0.2
    max_tokens: 1024
    description: "Assess content coverage and identify gaps"
    notes: "Needs to compare content against multiple questions"

  # Search Planner - creates multi-phase search plans
  planner:
    model: "qwen3:8b"
    context_window: 40960
    temperature: 0.4
    max_tokens: 2048  # Increased from 1024 - multi-phase plans need more space
    description: "Generate search plans with decomposed questions"
    notes: "Needs reasoning ability for query decomposition"

  # Content Synthesis - generates final answers
  synthesizer:
    model: "ministral-3:3b"
    context_window: 32000
    temperature: 0.6
    max_tokens: 8192  # Increased from 4096 - detailed troubleshooting needs 4K+ tokens
    description: "Synthesize final response from sources"
    notes: "Scored 0.848 in benchmarks, good balance of speed/quality"

  # Complex Reasoning - for troubleshooting, root cause analysis
  thinking:
    model: "ministral-3:3b"
    context_window: 32000
    temperature: 0.6
    max_tokens: 16384  # Increased from 8192 - DeepSeek R1 thinking routinely uses 10K+
    description: "Extended reasoning for complex queries"
    notes: "Used when requires_thinking_model=true"

  # CRAG Retrieval Evaluator
  # Benchmark: qwen3:4b-instruct-q8_0 (0.83 acc, 4513ms) = gemma3:4b (0.83 acc, 5079ms)
  retrieval_evaluator:
    model: "qwen3:4b-instruct-2507-q8_0"
    context_window: 32768
    temperature: 0.2
    max_tokens: 1024
    description: "Evaluate retrieval quality (CRAG)"
    notes: "Benchmark winner: fastest at 0.83 accuracy"

  # Self-Reflection - post-synthesis quality check
  # Benchmark: cogito:8b (1.0 acc, 5018ms) vs 4B models (0.50 acc - FAIL)
  self_reflection:
    model: "cogito:8b"
    context_window: 32768
    temperature: 0.3
    max_tokens: 1024
    description: "Post-synthesis quality evaluation"
    notes: "Benchmark winner: thinking model achieves 1.0 acc. 4B models fail (0.50)"

  # Verification - fact checking
  # Benchmark: gemma3:4b (0.75 acc, 5411ms) = qwen3:8b (0.75 acc, 20634ms)
  verifier:
    model: "gemma3:4b"
    context_window: 131072
    temperature: 0.2
    max_tokens: 1024
    description: "Verify claims against sources"
    notes: "Benchmark winner: same accuracy as qwen3:8b at 3.8x speed"

# =============================================================================
# UTILITY MODELS
# Specialized models for specific tasks
# =============================================================================

utility:
  # Entity Extraction - GSW-style entity tracking
  entity_extractor:
    model: "qwen3:8b"
    context_window: 40960
    temperature: 0.3
    max_tokens: 2048
    description: "Extract entities from content"

  # Query Tree Decomposition
  query_decomposer:
    model: "gemma3:4b"
    context_window: 131072
    temperature: 0.4
    max_tokens: 1024
    description: "Decompose queries into sub-queries"
    notes: "Fast model for simple decomposition"

  # Information Bottleneck Scoring
  relevance_scorer:
    model: "gemma3:4b"
    context_window: 131072
    temperature: 0.2
    max_tokens: 768  # Increased from 512 - JSON with reasoning needs more space
    description: "Score passage relevance"
    notes: "Simple scoring task - fast model preferred"

  # FLARE Uncertainty Detection
  uncertainty_detector:
    model: "qwen3:8b"
    context_window: 40960
    temperature: 0.3
    max_tokens: 512
    description: "Detect uncertainty in synthesis"
    notes: "Triggers proactive retrieval"

  # Experience Distillation
  experience_distiller:
    model: "qwen3:8b"
    context_window: 40960
    temperature: 0.4
    max_tokens: 2048
    description: "Distill search experiences into templates"

  # Prompt Compression
  prompt_compressor:
    model: "qwen2.5:0.5b"
    context_window: 32000
    temperature: 0.1
    max_tokens: 256
    description: "Score tokens for compression"
    notes: "Tiny model for fast scoring"

  # RAPTOR Summarization
  raptor_summarizer:
    model: "qwen3:8b"
    context_window: 40960
    temperature: 0.5
    max_tokens: 1024
    description: "Hierarchical document summarization"

  # NanoGraphRAG Extraction
  graph_extractor:
    model: "qwen3:8b"
    context_window: 40960
    temperature: 0.3
    max_tokens: 2048
    description: "Extract entities and relations for graph"

  # Graph Summary Generation
  graph_summarizer:
    model: "gemma3:4b"
    context_window: 131072
    temperature: 0.5
    max_tokens: 1024
    description: "Summarize graph communities"
    notes: "Fast model for bulk summarization"

  # Cross-Domain Validation
  cross_domain_validator:
    model: "qwen3:8b"
    context_window: 40960
    temperature: 0.2
    max_tokens: 1024
    description: "Validate cross-domain claims"

  # Entity Grounding
  entity_grounder:
    model: "qwen3:8b"
    context_window: 40960
    temperature: 0.2
    max_tokens: 1024
    description: "Verify entity references against sources"

  # Self-Discover Reasoning Composer
  reasoning_composer:
    model: "qwen3:8b"
    context_window: 40960
    temperature: 0.7
    max_tokens: 2048
    description: "Compose task-specific reasoning strategies"
    notes: "Self-Discover pattern for meta-reasoning"

  # DAG-Based Reasoning (Graph of Thoughts)
  reasoning_dag:
    model: "qwen3:8b"
    context_window: 40960
    temperature: 0.6
    max_tokens: 4096  # Increased from 2048 - multi-path reasoning generates substantial output
    description: "Multi-path reasoning via graph exploration"
    notes: "GoT-style branching and aggregation"

  # Pre-Act Planning (Enhanced Reasoning)
  enhanced_planner:
    model: "qwen3:8b"
    context_window: 40960
    temperature: 0.4
    max_tokens: 1024
    description: "Pre-Act multi-step planning"
    notes: "70% accuracy improvement over ReAct"

  # Pre-Act Reflection (Enhanced Reasoning)
  enhanced_reflector:
    model: "qwen3:8b"
    context_window: 40960
    temperature: 0.3
    max_tokens: 1024
    description: "Self-reflection after synthesis"

  # Cross-Encoder Reranking
  cross_encoder:
    model: "qwen3:8b"
    context_window: 40960
    temperature: 0.2
    max_tokens: 512
    description: "Neural reranking of search results"

  # HyDE Hypothetical Document Generation
  hyde_generator:
    model: "gemma3:4b"
    context_window: 131072
    temperature: 0.7
    max_tokens: 1024
    description: "Generate hypothetical documents for dense retrieval"
    notes: "Fast model for speculative generation"

  # FLARE Uncertainty Detection
  flare_detector:
    model: "qwen3:8b"
    context_window: 40960
    temperature: 0.3
    max_tokens: 512
    description: "Detect uncertainty for proactive retrieval"

  # Information Bottleneck Scoring
  information_bottleneck:
    model: "gemma3:4b"
    context_window: 131072
    temperature: 0.2
    max_tokens: 1024  # Increased from 512 - analysis output needs more space
    description: "Score passage relevance via information bottleneck"
    notes: "Fast model for scoring"

  # Sufficient Context Evaluator
  sufficient_context:
    model: "qwen3:8b"
    context_window: 40960
    temperature: 0.3
    max_tokens: 1024
    description: "Evaluate if context is sufficient to answer"

  # Self-Consistency Reasoning
  self_consistency:
    model: "gemma3:4b"
    context_window: 131072
    temperature: 0.7
    max_tokens: 1024
    description: "Multiple reasoning paths for consistency check"
    notes: "Higher temp for diverse paths"

  # Speculative RAG Verifier
  speculative_verifier:
    model: "deepseek-r1:14b-qwen-distill-q8_0"
    context_window: 32000
    temperature: 0.2
    max_tokens: 2048
    description: "Verify speculative generations"
    notes: "Larger model for accurate verification"

  # RAGAS Judge
  ragas_judge:
    model: "gemma3:4b"
    context_window: 131072
    temperature: 0.2
    max_tokens: 1024  # Increased from 512 - detailed evaluation metrics need more space
    description: "RAGAS-style evaluation metrics"

  # Entropy Monitor
  entropy_monitor:
    model: "qwen3:8b"
    context_window: 40960
    temperature: 0.3
    max_tokens: 512
    description: "Monitor token entropy for uncertainty"

  # Content Scraper Analysis
  scraper_analyzer:
    model: "qwen3:14b"
    context_window: 40960
    temperature: 0.3
    max_tokens: 4096
    description: "Analyze scraped content relevance"
    notes: "Larger model for nuanced content analysis"

  # Actor Factory (AIME-style)
  actor_factory:
    model: "qwen3:8b"
    context_window: 40960
    temperature: 0.4
    max_tokens: 1024
    description: "Dynamic agent assembly"

  # Adaptive Refinement
  adaptive_refinement:
    model: "qwen3:8b"
    context_window: 40960
    temperature: 0.4
    max_tokens: 1024
    description: "Adaptive query refinement based on results"

  # Information Gain Scoring
  information_gain:
    model: "gemma3:4b"
    context_window: 131072
    temperature: 0.2
    max_tokens: 512
    description: "Score information gain from new sources"

# =============================================================================
# EMBEDDING MODELS
# =============================================================================

embeddings:
  # Primary embedding model
  primary:
    model: "qwen3-embedding:latest"
    dimensions: 1024
    description: "Primary text embeddings"

  # Semantic cache embeddings
  cache:
    model: "nomic-embed-text"
    dimensions: 768
    description: "Fast embeddings for semantic cache"

  # Shadow embeddings (challenger)
  shadow:
    model: "qwen3-embedding:0.6b"
    dimensions: 512
    description: "Lightweight challenger for A/B testing"

# =============================================================================
# CORPUS BUILDING MODELS
# Used for offline document processing
# =============================================================================

corpus:
  # PLC/Automation corpus extraction
  plc_extractor:
    model: "qwen3:8b"
    temperature: 0.3
    max_tokens: 2048
    description: "Extract fault codes and procedures from PLC docs"

  # IMM corpus extraction
  imm_extractor:
    model: "gemma3:4b"
    temperature: 0.3
    max_tokens: 2048
    description: "Extract injection molding machine info"
    notes: "Fast model for bulk extraction"

  # Domain corpus processing
  domain_extractor:
    model: "qwen3:8b"
    temperature: 0.3
    max_tokens: 2048
    description: "General domain document extraction"

# =============================================================================
# PRESETS (Updated from Benchmark Results 2026-01-12)
# Quick model assignment profiles for different use cases
# =============================================================================

presets:
  # Maximum speed - use fastest models from benchmarks
  # Total VRAM: ~10GB | Avg latency: ~4.5s per agent
  speed:
    pipeline.analyzer: "qwen3:4b-instruct-2507-q8_0"       # 4235ms, 0.70 acc
    pipeline.url_evaluator: "qwen3:4b-instruct-2507-q8_0"  # 4515ms, 0.66 acc
    pipeline.coverage_evaluator: "qwen3:4b-instruct-2507-q8_0"
    pipeline.planner: "qwen3:4b-instruct-2507-q8_0"
    pipeline.synthesizer: "qwen3:8b"                       # Keep quality for output
    pipeline.thinking: "qwen3:8b"
    pipeline.retrieval_evaluator: "qwen3:4b-instruct-2507-q8_0"  # 4513ms, 0.83 acc
    pipeline.self_reflection: "cogito:8b"                  # 5018ms, 1.0 acc (4B fails!)
    pipeline.verifier: "qwen3:4b-instruct-2507-q8_0"       # 4623ms, 0.70 acc

  # Maximum quality - use best accuracy models from benchmarks
  # Total VRAM: ~24GB | Avg latency: ~11s per agent
  quality:
    pipeline.analyzer: "qwen3:30b-a3b-instruct-2507-q4_K_M"  # 10981ms, 0.88 acc
    pipeline.url_evaluator: "qwen3:30b-a3b-instruct-2507-q4_K_M"  # 10423ms, 1.0 acc!
    pipeline.coverage_evaluator: "qwen3:30b-a3b-instruct-2507-q4_K_M"
    pipeline.planner: "qwen3:30b-a3b-instruct-2507-q4_K_M"
    pipeline.synthesizer: "qwen3:30b-a3b-instruct-2507-q4_K_M"
    pipeline.thinking: "deepseek-r1:14b-qwen-distill-q8_0"
    pipeline.retrieval_evaluator: "qwen3:30b-a3b-instruct-2507-q4_K_M"  # 10197ms, 0.83 acc
    pipeline.self_reflection: "qwen3:30b-a3b-instruct-2507-q4_K_M"  # 11696ms, 1.0 acc
    pipeline.verifier: "gemma3:4b"                          # 5411ms, 0.75 acc (best)

  # Balanced (default) - benchmark-optimized production config
  # Total VRAM: ~12GB | Best accuracy/speed tradeoff
  balanced:
    pipeline.analyzer: "gemma3:4b"                         # 5091ms, 0.91 acc
    pipeline.url_evaluator: "gemma3:4b"                    # 5712ms, 0.66 acc
    pipeline.coverage_evaluator: "qwen3:8b"
    pipeline.planner: "qwen3:8b"
    pipeline.synthesizer: "qwen3:8b"
    pipeline.thinking: "qwen3:8b"
    pipeline.retrieval_evaluator: "qwen3:4b-instruct-2507-q8_0"  # 4513ms, 0.83 acc
    pipeline.self_reflection: "cogito:8b"                  # 5018ms, 1.0 acc
    pipeline.verifier: "gemma3:4b"                         # 5411ms, 0.75 acc

  # Low VRAM - for <12GB GPU environments
  # Total VRAM: ~5GB | Single model loaded at a time
  low_vram:
    pipeline.analyzer: "qwen3:4b-instruct-2507-q8_0"
    pipeline.url_evaluator: "qwen3:4b-instruct-2507-q8_0"
    pipeline.coverage_evaluator: "qwen3:4b-instruct-2507-q8_0"
    pipeline.planner: "qwen3:4b-instruct-2507-q8_0"
    pipeline.synthesizer: "qwen3:4b-instruct-2507-q8_0"
    pipeline.thinking: "qwen3:4b-instruct-2507-q8_0"
    pipeline.retrieval_evaluator: "qwen3:4b-instruct-2507-q8_0"
    pipeline.self_reflection: "qwen3:4b-instruct-2507-q8_0"  # Note: 0.50 acc - not ideal
    pipeline.verifier: "qwen3:4b-instruct-2507-q8_0"

# =============================================================================
# BENCHMARK RESULTS REFERENCE (2026-01-12)
# From tests/data/agent_benchmarks.db
# =============================================================================

benchmarks:
  # Analyzer benchmarks (query classification)
  analyzer:
    - model: "qwen3:4b-instruct-2507-q8_0"
      avg_duration_ms: 4235
      accuracy: 0.70
      vram_mb: 4996
      note: "Fastest model"
    - model: "gemma3:4b"
      avg_duration_ms: 5091
      accuracy: 0.91
      vram_mb: 2904
      note: "Best accuracy/speed balance"
    - model: "qwen3:30b-a3b-instruct-2507-q4_K_M"
      avg_duration_ms: 10981
      accuracy: 0.88
      vram_mb: 18260
      note: "Quality option"

  # CRAG Evaluator benchmarks (retrieval quality)
  crag_evaluator:
    - model: "qwen3:4b-instruct-2507-q8_0"
      avg_duration_ms: 4513
      accuracy: 0.83
      vram_mb: 4996
      note: "Fastest with best accuracy"
    - model: "gemma3:4b"
      avg_duration_ms: 5079
      accuracy: 0.83
      vram_mb: 3875
      note: "Equal accuracy, slightly slower"

  # Self-Reflection benchmarks (synthesis evaluation)
  self_reflection:
    - model: "cogito:8b"
      avg_duration_ms: 5018
      accuracy: 1.00
      vram_mb: 3496
      note: "Best: thinking model achieves perfect accuracy"
    - model: "qwen3:30b-a3b-instruct-2507-q4_K_M"
      avg_duration_ms: 11696
      accuracy: 1.00
      vram_mb: 18260
      note: "Quality alternative"
    - model: "qwen3:4b-instruct-2507-q8_0"
      avg_duration_ms: 5138
      accuracy: 0.50
      vram_mb: 4996
      note: "WARNING: 4B models fail at this task"

  # URL Relevance Filter benchmarks
  url_filter:
    - model: "qwen3:30b-a3b-instruct-2507-q4_K_M"
      avg_duration_ms: 10423
      accuracy: 1.00
      vram_mb: 18266
      note: "PERFECT accuracy - critical for avoiding wasted scrapes"
    - model: "qwen3:4b-instruct-2507-q8_0"
      avg_duration_ms: 4515
      accuracy: 0.66
      vram_mb: 4996
      note: "Fastest, 100% success rate"
    - model: "gemma3:4b"
      avg_duration_ms: 5712
      accuracy: 0.66
      vram_mb: 1570
      note: "Low VRAM option"

  # Verifier benchmarks (claim verification)
  verifier:
    - model: "gemma3:4b"
      avg_duration_ms: 5411
      accuracy: 0.75
      vram_mb: 3952
      note: "Best accuracy at fastest speed"
    - model: "qwen3:4b-instruct-2507-q8_0"
      avg_duration_ms: 4623
      accuracy: 0.70
      vram_mb: 4996
      note: "Fastest but slightly lower accuracy"

  # Legacy synthesis benchmarks
  synthesis:
    - model: "gemma3:4b"
      avg_duration_ms: 14305
      ttfs_ms: 4175
      success_rate: 1.0
      vram_gb: 3.3
    - model: "qwen3:8b"
      avg_duration_ms: 28344
      ttfs_ms: 13033
      success_rate: 1.0
      vram_gb: 5.2
